{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2681cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression Report ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Logistic Regression Report ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m y_pred_lr = \u001b[43mmodel\u001b[49m.predict(x_test)\n\u001b[32m      8\u001b[39m target_names = [\u001b[33m'\u001b[39m\u001b[33mspam\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcomplaint\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfeedback\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# Based on mapping: 0, 1, 2, 3\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Precision: Correct predictions / Total predicted\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Recall: Actual cases found / Total actual cases\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# MILESTONE 2: EMAIL CATEGORIZATION ENGINE\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING MILESTONE 2: MODEL TRAINING & EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Necessary Imports\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define target names for reports (Must match your label map order: 0, 1, 2, 3)\n",
    "target_names = ['spam', 'complaint', 'request', 'feedback'] \n",
    "\n",
    "# -------------------------------------------------------\n",
    "# TASK 1: TRAIN BASELINE CLASSIFIERS\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# --- A. Logistic Regression ---\n",
    "print(\"\\n--- 1. Training Logistic Regression ---\")\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "lr_pred = lr_model.predict(x_test)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, lr_pred):.4f}\")\n",
    "print(\"Classification Report (LR):\")\n",
    "print(classification_report(y_test, lr_pred, target_names=target_names))\n",
    "\n",
    "# --- B. Naive Bayes ---\n",
    "print(\"\\n--- 2. Training Naive Bayes ---\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(x_train, y_train)\n",
    "\n",
    "nb_pred = nb_model.predict(x_test)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, nb_pred):.4f}\")\n",
    "print(\"Classification Report (NB):\")\n",
    "print(classification_report(y_test, nb_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# TASK 2: FINE-TUNE TRANSFORMER MODEL (DistilBERT)\n",
    "# -------------------------------------------------------\n",
    "print(\"\\n--- 3. Fine-tuning DistilBERT (Transformer) ---\")\n",
    "\n",
    "# 1. Prepare Data for BERT\n",
    "label_map = {'spam': 0, 'complaint': 1, 'request': 2, 'feedback': 3}\n",
    "\n",
    "bert_texts = df_final['Cleaned_Text'].tolist()\n",
    "bert_labels = df_final['category'].map(label_map).tolist()\n",
    "\n",
    "# Split data (using lowercase names to match your previous code style)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    bert_texts, bert_labels, test_size=0.2, random_state=3\n",
    ")\n",
    "\n",
    "# 2. Tokenization\n",
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(\"Tokenizing data...\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# 3. Create Dataset Class\n",
    "class EmailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EmailDataset(train_encodings, train_labels)\n",
    "val_dataset = EmailDataset(val_encodings, val_labels)\n",
    "\n",
    "# 4. Load Model\n",
    "print(\"Loading Model...\")\n",
    "model_bert = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "# 5. Training Setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",  \n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# 6. Train\n",
    "print(\"Starting Training (Fine-tuning)...\")\n",
    "trainer.train()\n",
    "\n",
    "# 7. Evaluate DistilBERT (Detailed Report)\n",
    "print(\"\\n--- DistilBERT Evaluation ---\")\n",
    "\n",
    "# Predict on the validation set\n",
    "predictions = trainer.predict(val_dataset)\n",
    "# Convert logits to class IDs\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Print the full Classification Report\n",
    "print(f\"DistilBERT Accuracy: {accuracy_score(val_labels, preds):.4f}\")\n",
    "print(\"Classification Report (DistilBERT):\")\n",
    "print(classification_report(val_labels, preds, target_names=target_names))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# TASK 3: PREDICTION TEST (REAL DATA)\n",
    "# -------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION TEST ON RANDOM DATASET SAMPLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Pick a random row\n",
    "random_index = random.randint(0, len(df_final) - 1)\n",
    "sample_row = df_final.iloc[random_index]\n",
    "\n",
    "input_text = sample_row['text']       \n",
    "true_label = sample_row['category']   \n",
    "\n",
    "# 2. Preprocess\n",
    "cleaned_input = clean_email(input_text)\n",
    "\n",
    "# 3. Predict with DistilBERT\n",
    "inputs = tokenizer(cleaned_input, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "inputs = {k: v.to(model_bert.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_bert(**inputs)\n",
    "\n",
    "predicted_class_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "predicted_result = reverse_label_map[predicted_class_id]\n",
    "\n",
    "# 4. Display Results\n",
    "print(f\"Input Email :\\n'{input_text}'\\n\")\n",
    "print(f\"Predicted Label : {predicted_result.upper()}\")\n",
    "print(f\"Actual Label : {true_label.upper()}\")\n",
    "\n",
    "if true_label == predicted_result:\n",
    "    print(\"\\n Result: ACCURATE\")\n",
    "else:\n",
    "    print(\"\\n Result: INCORRECT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
